{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tradução Automática\n",
        "\n",
        "Script que realiza a tradução de uma sentença do português para o inglês por meio de um modelo Seq2Seq.\n",
        "\n",
        "Inspiração: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
      ],
      "metadata": {
        "id": "8x8BHDrO39Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random"
      ],
      "metadata": {
        "id": "zY7LmjBq1x_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação do corpus paralelo e customizado para tradução automática"
      ],
      "metadata": {
        "id": "sr70BiZV4FcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab,max_len):\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "        self.source_vocab = source_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_sentence = self.source_sentences[index]\n",
        "        target_sentence = self.target_sentences[index]\n",
        "\n",
        "        source_indices = [self.source_vocab.get(word, self.source_vocab['<unk>']) for word in source_sentence.split()]\n",
        "        target_indices = [self.target_vocab.get(word, self.target_vocab['<unk>']) for word in target_sentence.split()]\n",
        "\n",
        "        source_indices.append(self.source_vocab['<sos>'])\n",
        "        target_indices.append(self.target_vocab['<sos>'])\n",
        "\n",
        "        tam = self.max_len -len(source_indices)-1\n",
        "        source_indices.extend([self.source_vocab['<pad>']]*tam)\n",
        "        tam = self.max_len -len(target_indices)-1\n",
        "        target_indices.extend([self.target_vocab['<pad>']]*tam)\n",
        "\n",
        "        source_indices.extend([self.source_vocab['<eos>']])\n",
        "        source_sentence_tensor = torch.LongTensor(source_indices)\n",
        "\n",
        "        target_indices.extend([self.target_vocab['<eos>']])\n",
        "        target_sentence_tensor = torch.LongTensor(target_indices)\n",
        "\n",
        "        return source_sentence_tensor,target_sentence_tensor"
      ],
      "metadata": {
        "id": "9_2tjT7X14FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim,embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "        embedding = self.embedding(x)\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "6sjuSf9M2K0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    # formato de entrada: [batch_size], recebe uma palavra (apenas a primeira) da sentenca de cada batch\n",
        "    def forward(self, x, hidden, cell):\n",
        "\n",
        "        # se batch_size = 4, entra [1,2,3,4] sai [[1],[2],[3],[4]]\n",
        "        #print(x.shape)\n",
        "        x = x.unsqueeze(0)\n",
        "        #print(x.shape)\n",
        "\n",
        "        # estrutura de saida do embedding possui dimensoes: (1, batch_size, embedding_dim)\n",
        "        embedding = self.embedding(x)\n",
        "\n",
        "        # estrutura de saida do LSTM possui dimensoes: (1, batch_size, hidden_dim)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "\n",
        "        # estrutura de saida do LSTM possui dimensoes: (1, batch_size, hidden_dim)\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # estrutura de saida [[1],[2],[3],[4]] sai se batch_size = 4, entra [1,2,3,4] sai\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden, cell"
      ],
      "metadata": {
        "id": "LFVC3EA22MSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(target_vocab)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Pega o ultimo [estado interno, estado de celula] do enconder como o vetor de contexto na entrada do decoder\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Trabalhando como um modelo de linguagem\n",
        "            # Armazena a proxima palavra a ser predita\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Pega a palavra com a maior probabilidade a ser predita\n",
        "            next_word = output.argmax(1)\n",
        "\n",
        "            #if random.random() < teacher_force_ratio:\n",
        "            x = target[t]\n",
        "            #else:\n",
        "            #x = next_word\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "drA-G2RL2OAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição dos dados de treinamento"
      ],
      "metadata": {
        "id": "XyMI6A8q_x9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    (\"eu gosto de python\", \"i like python too\"),\n",
        "    (\"eu nao sei\",\"i do not know\"),\n",
        "    (\"eu estou sem palavras\",\"i am speechless\"),\n",
        "    (\"sou eu quem faz o almoco\",\"it is me who cooks the lunch\"),\n",
        "    (\"eu posso montar o armario\",\"i can build the shelve\"),\n",
        "    (\"voce nao sabe como falar isso em ingles\",\"you do not know how to say that in english\")\n",
        "]"
      ],
      "metadata": {
        "id": "Hb-STvEk_yFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação do vocabulario como dicionarios"
      ],
      "metadata": {
        "id": "ZES8iwj4-aed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "target_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "\n",
        "for pt_br,eng in training_data:\n",
        "    for word in pt_br.split():\n",
        "        if word not in source_vocab:\n",
        "            source_vocab[word] = len(source_vocab)\n",
        "    for word in eng.split():\n",
        "        if word not in target_vocab:\n",
        "            target_vocab[word] = len(target_vocab)\n",
        "\n",
        "source_idx2word = {}\n",
        "target_idx2word = {}\n",
        "\n",
        "for word,index in source_vocab.items():\n",
        "    source_idx2word[index] = word\n",
        "\n",
        "for word,index in target_vocab.items():\n",
        "    target_idx2word[index] = word"
      ],
      "metadata": {
        "id": "WSmrUqDL_zhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição dos hiperparâmetros"
      ],
      "metadata": {
        "id": "AYhHxXVvq4bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim_encoder = len(source_vocab)\n",
        "input_dim_decoder = len(target_vocab)\n",
        "hidden_dim = 256\n",
        "output_dim = len(target_vocab)\n",
        "\n",
        "embedding_dim = 128\n",
        "learning_rate = 1e-5\n",
        "batch_size=2\n",
        "num_epochs = 300\n",
        "max_sentence_len = 16"
      ],
      "metadata": {
        "id": "DVf_iNFg2Pjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar o dataloader a partir dos dados de treinamento"
      ],
      "metadata": {
        "id": "kHEgXUSy-lRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset([pair[0] for pair in training_data], [pair[1] for pair in training_data], source_vocab, target_vocab,max_sentence_len)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "p9MfBhI62Srs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_dim_encoder, embedding_dim, hidden_dim)\n",
        "decoder = Decoder(input_dim_decoder, embedding_dim, hidden_dim,output_dim)\n",
        "model = Seq2Seq(encoder, decoder)"
      ],
      "metadata": {
        "id": "uhFDnGLd2TCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignorar indice do padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "gbcmEXjI2Wn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processo de treinamento"
      ],
      "metadata": {
        "id": "Bwnko6q3-1J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # ativando o modo de treinamento do modelo\n",
        "    model.train()\n",
        "\n",
        "    for inp_data,target in train_loader:\n",
        "\n",
        "        # zera os gradientes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # passo forward: obtem as saidas para as sentenças do batch\n",
        "        output = model(inp_data, target)\n",
        "\n",
        "        # Saida possui dimensoes (target_len, batch_size, output_dim), mas a Cross Entropy Loss\n",
        "        # processa entradas apenas nas dimensoes (output_words * batch_size)\n",
        "        # Assim eh necessario fazer um reshape. Retire os comentarios dos prints\n",
        "        # para verificar\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss+=loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}] ------------------------------- {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imSLL3To2YWs",
        "outputId": "70c23d7c-d95f-4390-82f7-5c7b5f5223e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 300] ------------------------------- 3.4528\n",
            "[Epoch 1 / 300] ------------------------------- 3.4444\n",
            "[Epoch 2 / 300] ------------------------------- 3.4417\n",
            "[Epoch 3 / 300] ------------------------------- 3.4434\n",
            "[Epoch 4 / 300] ------------------------------- 3.4409\n",
            "[Epoch 5 / 300] ------------------------------- 3.4394\n",
            "[Epoch 6 / 300] ------------------------------- 3.4354\n",
            "[Epoch 7 / 300] ------------------------------- 3.4278\n",
            "[Epoch 8 / 300] ------------------------------- 3.4239\n",
            "[Epoch 9 / 300] ------------------------------- 3.4305\n",
            "[Epoch 10 / 300] ------------------------------- 3.4137\n",
            "[Epoch 11 / 300] ------------------------------- 3.4144\n",
            "[Epoch 12 / 300] ------------------------------- 3.4226\n",
            "[Epoch 13 / 300] ------------------------------- 3.4096\n",
            "[Epoch 14 / 300] ------------------------------- 3.4112\n",
            "[Epoch 15 / 300] ------------------------------- 3.4001\n",
            "[Epoch 16 / 300] ------------------------------- 3.4054\n",
            "[Epoch 17 / 300] ------------------------------- 3.3973\n",
            "[Epoch 18 / 300] ------------------------------- 3.3913\n",
            "[Epoch 19 / 300] ------------------------------- 3.4006\n",
            "[Epoch 20 / 300] ------------------------------- 3.3982\n",
            "[Epoch 21 / 300] ------------------------------- 3.3854\n",
            "[Epoch 22 / 300] ------------------------------- 3.3872\n",
            "[Epoch 23 / 300] ------------------------------- 3.3908\n",
            "[Epoch 24 / 300] ------------------------------- 3.3973\n",
            "[Epoch 25 / 300] ------------------------------- 3.3829\n",
            "[Epoch 26 / 300] ------------------------------- 3.3848\n",
            "[Epoch 27 / 300] ------------------------------- 3.3897\n",
            "[Epoch 28 / 300] ------------------------------- 3.3861\n",
            "[Epoch 29 / 300] ------------------------------- 3.3650\n",
            "[Epoch 30 / 300] ------------------------------- 3.3722\n",
            "[Epoch 31 / 300] ------------------------------- 3.3676\n",
            "[Epoch 32 / 300] ------------------------------- 3.3611\n",
            "[Epoch 33 / 300] ------------------------------- 3.3641\n",
            "[Epoch 34 / 300] ------------------------------- 3.3746\n",
            "[Epoch 35 / 300] ------------------------------- 3.3666\n",
            "[Epoch 36 / 300] ------------------------------- 3.3606\n",
            "[Epoch 37 / 300] ------------------------------- 3.3597\n",
            "[Epoch 38 / 300] ------------------------------- 3.3461\n",
            "[Epoch 39 / 300] ------------------------------- 3.3531\n",
            "[Epoch 40 / 300] ------------------------------- 3.3598\n",
            "[Epoch 41 / 300] ------------------------------- 3.3511\n",
            "[Epoch 42 / 300] ------------------------------- 3.3574\n",
            "[Epoch 43 / 300] ------------------------------- 3.3449\n",
            "[Epoch 44 / 300] ------------------------------- 3.3423\n",
            "[Epoch 45 / 300] ------------------------------- 3.3324\n",
            "[Epoch 46 / 300] ------------------------------- 3.3322\n",
            "[Epoch 47 / 300] ------------------------------- 3.3494\n",
            "[Epoch 48 / 300] ------------------------------- 3.3455\n",
            "[Epoch 49 / 300] ------------------------------- 3.3269\n",
            "[Epoch 50 / 300] ------------------------------- 3.3355\n",
            "[Epoch 51 / 300] ------------------------------- 3.3255\n",
            "[Epoch 52 / 300] ------------------------------- 3.3149\n",
            "[Epoch 53 / 300] ------------------------------- 3.3176\n",
            "[Epoch 54 / 300] ------------------------------- 3.3203\n",
            "[Epoch 55 / 300] ------------------------------- 3.3150\n",
            "[Epoch 56 / 300] ------------------------------- 3.3214\n",
            "[Epoch 57 / 300] ------------------------------- 3.3111\n",
            "[Epoch 58 / 300] ------------------------------- 3.3104\n",
            "[Epoch 59 / 300] ------------------------------- 3.3028\n",
            "[Epoch 60 / 300] ------------------------------- 3.3067\n",
            "[Epoch 61 / 300] ------------------------------- 3.3144\n",
            "[Epoch 62 / 300] ------------------------------- 3.3043\n",
            "[Epoch 63 / 300] ------------------------------- 3.3020\n",
            "[Epoch 64 / 300] ------------------------------- 3.2910\n",
            "[Epoch 65 / 300] ------------------------------- 3.2880\n",
            "[Epoch 66 / 300] ------------------------------- 3.3062\n",
            "[Epoch 67 / 300] ------------------------------- 3.2707\n",
            "[Epoch 68 / 300] ------------------------------- 3.2861\n",
            "[Epoch 69 / 300] ------------------------------- 3.2915\n",
            "[Epoch 70 / 300] ------------------------------- 3.2785\n",
            "[Epoch 71 / 300] ------------------------------- 3.2910\n",
            "[Epoch 72 / 300] ------------------------------- 3.2936\n",
            "[Epoch 73 / 300] ------------------------------- 3.2683\n",
            "[Epoch 74 / 300] ------------------------------- 3.2714\n",
            "[Epoch 75 / 300] ------------------------------- 3.2723\n",
            "[Epoch 76 / 300] ------------------------------- 3.2500\n",
            "[Epoch 77 / 300] ------------------------------- 3.2617\n",
            "[Epoch 78 / 300] ------------------------------- 3.2343\n",
            "[Epoch 79 / 300] ------------------------------- 3.2630\n",
            "[Epoch 80 / 300] ------------------------------- 3.2554\n",
            "[Epoch 81 / 300] ------------------------------- 3.2422\n",
            "[Epoch 82 / 300] ------------------------------- 3.2721\n",
            "[Epoch 83 / 300] ------------------------------- 3.2701\n",
            "[Epoch 84 / 300] ------------------------------- 3.2412\n",
            "[Epoch 85 / 300] ------------------------------- 3.2544\n",
            "[Epoch 86 / 300] ------------------------------- 3.2411\n",
            "[Epoch 87 / 300] ------------------------------- 3.2385\n",
            "[Epoch 88 / 300] ------------------------------- 3.2479\n",
            "[Epoch 89 / 300] ------------------------------- 3.2133\n",
            "[Epoch 90 / 300] ------------------------------- 3.2357\n",
            "[Epoch 91 / 300] ------------------------------- 3.2061\n",
            "[Epoch 92 / 300] ------------------------------- 3.2046\n",
            "[Epoch 93 / 300] ------------------------------- 3.2115\n",
            "[Epoch 94 / 300] ------------------------------- 3.2430\n",
            "[Epoch 95 / 300] ------------------------------- 3.2204\n",
            "[Epoch 96 / 300] ------------------------------- 3.1995\n",
            "[Epoch 97 / 300] ------------------------------- 3.1955\n",
            "[Epoch 98 / 300] ------------------------------- 3.2265\n",
            "[Epoch 99 / 300] ------------------------------- 3.2054\n",
            "[Epoch 100 / 300] ------------------------------- 3.1945\n",
            "[Epoch 101 / 300] ------------------------------- 3.1762\n",
            "[Epoch 102 / 300] ------------------------------- 3.2131\n",
            "[Epoch 103 / 300] ------------------------------- 3.2145\n",
            "[Epoch 104 / 300] ------------------------------- 3.1711\n",
            "[Epoch 105 / 300] ------------------------------- 3.2165\n",
            "[Epoch 106 / 300] ------------------------------- 3.1556\n",
            "[Epoch 107 / 300] ------------------------------- 3.1983\n",
            "[Epoch 108 / 300] ------------------------------- 3.1747\n",
            "[Epoch 109 / 300] ------------------------------- 3.1562\n",
            "[Epoch 110 / 300] ------------------------------- 3.1720\n",
            "[Epoch 111 / 300] ------------------------------- 3.1610\n",
            "[Epoch 112 / 300] ------------------------------- 3.1553\n",
            "[Epoch 113 / 300] ------------------------------- 3.1571\n",
            "[Epoch 114 / 300] ------------------------------- 3.1658\n",
            "[Epoch 115 / 300] ------------------------------- 3.1509\n",
            "[Epoch 116 / 300] ------------------------------- 3.1727\n",
            "[Epoch 117 / 300] ------------------------------- 3.1770\n",
            "[Epoch 118 / 300] ------------------------------- 3.1440\n",
            "[Epoch 119 / 300] ------------------------------- 3.1565\n",
            "[Epoch 120 / 300] ------------------------------- 3.1209\n",
            "[Epoch 121 / 300] ------------------------------- 3.1693\n",
            "[Epoch 122 / 300] ------------------------------- 3.1410\n",
            "[Epoch 123 / 300] ------------------------------- 3.1505\n",
            "[Epoch 124 / 300] ------------------------------- 3.1196\n",
            "[Epoch 125 / 300] ------------------------------- 3.1328\n",
            "[Epoch 126 / 300] ------------------------------- 3.1216\n",
            "[Epoch 127 / 300] ------------------------------- 3.1203\n",
            "[Epoch 128 / 300] ------------------------------- 3.1291\n",
            "[Epoch 129 / 300] ------------------------------- 3.1223\n",
            "[Epoch 130 / 300] ------------------------------- 3.1009\n",
            "[Epoch 131 / 300] ------------------------------- 3.1213\n",
            "[Epoch 132 / 300] ------------------------------- 3.1080\n",
            "[Epoch 133 / 300] ------------------------------- 3.0979\n",
            "[Epoch 134 / 300] ------------------------------- 3.1120\n",
            "[Epoch 135 / 300] ------------------------------- 3.0839\n",
            "[Epoch 136 / 300] ------------------------------- 3.0748\n",
            "[Epoch 137 / 300] ------------------------------- 3.0710\n",
            "[Epoch 138 / 300] ------------------------------- 3.0642\n",
            "[Epoch 139 / 300] ------------------------------- 3.0554\n",
            "[Epoch 140 / 300] ------------------------------- 3.0759\n",
            "[Epoch 141 / 300] ------------------------------- 3.0370\n",
            "[Epoch 142 / 300] ------------------------------- 3.0954\n",
            "[Epoch 143 / 300] ------------------------------- 3.0527\n",
            "[Epoch 144 / 300] ------------------------------- 3.0430\n",
            "[Epoch 145 / 300] ------------------------------- 3.1014\n",
            "[Epoch 146 / 300] ------------------------------- 3.1111\n",
            "[Epoch 147 / 300] ------------------------------- 3.0605\n",
            "[Epoch 148 / 300] ------------------------------- 3.1020\n",
            "[Epoch 149 / 300] ------------------------------- 3.0742\n",
            "[Epoch 150 / 300] ------------------------------- 3.0400\n",
            "[Epoch 151 / 300] ------------------------------- 3.0698\n",
            "[Epoch 152 / 300] ------------------------------- 3.0299\n",
            "[Epoch 153 / 300] ------------------------------- 2.9982\n",
            "[Epoch 154 / 300] ------------------------------- 3.0252\n",
            "[Epoch 155 / 300] ------------------------------- 3.0045\n",
            "[Epoch 156 / 300] ------------------------------- 3.0062\n",
            "[Epoch 157 / 300] ------------------------------- 3.0210\n",
            "[Epoch 158 / 300] ------------------------------- 3.0306\n",
            "[Epoch 159 / 300] ------------------------------- 2.9986\n",
            "[Epoch 160 / 300] ------------------------------- 3.0011\n",
            "[Epoch 161 / 300] ------------------------------- 2.9723\n",
            "[Epoch 162 / 300] ------------------------------- 2.9951\n",
            "[Epoch 163 / 300] ------------------------------- 2.9743\n",
            "[Epoch 164 / 300] ------------------------------- 2.9993\n",
            "[Epoch 165 / 300] ------------------------------- 2.9826\n",
            "[Epoch 166 / 300] ------------------------------- 3.0512\n",
            "[Epoch 167 / 300] ------------------------------- 2.9730\n",
            "[Epoch 168 / 300] ------------------------------- 3.0301\n",
            "[Epoch 169 / 300] ------------------------------- 2.9286\n",
            "[Epoch 170 / 300] ------------------------------- 2.9673\n",
            "[Epoch 171 / 300] ------------------------------- 2.9543\n",
            "[Epoch 172 / 300] ------------------------------- 2.9901\n",
            "[Epoch 173 / 300] ------------------------------- 2.9466\n",
            "[Epoch 174 / 300] ------------------------------- 2.9068\n",
            "[Epoch 175 / 300] ------------------------------- 2.9428\n",
            "[Epoch 176 / 300] ------------------------------- 2.9768\n",
            "[Epoch 177 / 300] ------------------------------- 2.9130\n",
            "[Epoch 178 / 300] ------------------------------- 2.9232\n",
            "[Epoch 179 / 300] ------------------------------- 2.9372\n",
            "[Epoch 180 / 300] ------------------------------- 2.9219\n",
            "[Epoch 181 / 300] ------------------------------- 3.0007\n",
            "[Epoch 182 / 300] ------------------------------- 2.9787\n",
            "[Epoch 183 / 300] ------------------------------- 2.8992\n",
            "[Epoch 184 / 300] ------------------------------- 2.8739\n",
            "[Epoch 185 / 300] ------------------------------- 2.9614\n",
            "[Epoch 186 / 300] ------------------------------- 2.9306\n",
            "[Epoch 187 / 300] ------------------------------- 2.9720\n",
            "[Epoch 188 / 300] ------------------------------- 2.9128\n",
            "[Epoch 189 / 300] ------------------------------- 2.9756\n",
            "[Epoch 190 / 300] ------------------------------- 2.9511\n",
            "[Epoch 191 / 300] ------------------------------- 2.8552\n",
            "[Epoch 192 / 300] ------------------------------- 2.8586\n",
            "[Epoch 193 / 300] ------------------------------- 2.8525\n",
            "[Epoch 194 / 300] ------------------------------- 2.9298\n",
            "[Epoch 195 / 300] ------------------------------- 2.8907\n",
            "[Epoch 196 / 300] ------------------------------- 2.8896\n",
            "[Epoch 197 / 300] ------------------------------- 2.8978\n",
            "[Epoch 198 / 300] ------------------------------- 2.8585\n",
            "[Epoch 199 / 300] ------------------------------- 2.8503\n",
            "[Epoch 200 / 300] ------------------------------- 2.8799\n",
            "[Epoch 201 / 300] ------------------------------- 2.9080\n",
            "[Epoch 202 / 300] ------------------------------- 2.8381\n",
            "[Epoch 203 / 300] ------------------------------- 2.9003\n",
            "[Epoch 204 / 300] ------------------------------- 2.9301\n",
            "[Epoch 205 / 300] ------------------------------- 2.8390\n",
            "[Epoch 206 / 300] ------------------------------- 2.8049\n",
            "[Epoch 207 / 300] ------------------------------- 2.8488\n",
            "[Epoch 208 / 300] ------------------------------- 2.7868\n",
            "[Epoch 209 / 300] ------------------------------- 2.7795\n",
            "[Epoch 210 / 300] ------------------------------- 2.8534\n",
            "[Epoch 211 / 300] ------------------------------- 2.8741\n",
            "[Epoch 212 / 300] ------------------------------- 2.8586\n",
            "[Epoch 213 / 300] ------------------------------- 2.7977\n",
            "[Epoch 214 / 300] ------------------------------- 2.8017\n",
            "[Epoch 215 / 300] ------------------------------- 2.8601\n",
            "[Epoch 216 / 300] ------------------------------- 2.8182\n",
            "[Epoch 217 / 300] ------------------------------- 2.7695\n",
            "[Epoch 218 / 300] ------------------------------- 2.8081\n",
            "[Epoch 219 / 300] ------------------------------- 2.7779\n",
            "[Epoch 220 / 300] ------------------------------- 2.7400\n",
            "[Epoch 221 / 300] ------------------------------- 2.8806\n",
            "[Epoch 222 / 300] ------------------------------- 2.7422\n",
            "[Epoch 223 / 300] ------------------------------- 2.7794\n",
            "[Epoch 224 / 300] ------------------------------- 2.8309\n",
            "[Epoch 225 / 300] ------------------------------- 2.7211\n",
            "[Epoch 226 / 300] ------------------------------- 2.7320\n",
            "[Epoch 227 / 300] ------------------------------- 2.8634\n",
            "[Epoch 228 / 300] ------------------------------- 2.7865\n",
            "[Epoch 229 / 300] ------------------------------- 2.8168\n",
            "[Epoch 230 / 300] ------------------------------- 2.8007\n",
            "[Epoch 231 / 300] ------------------------------- 2.7970\n",
            "[Epoch 232 / 300] ------------------------------- 2.6953\n",
            "[Epoch 233 / 300] ------------------------------- 2.7017\n",
            "[Epoch 234 / 300] ------------------------------- 2.7597\n",
            "[Epoch 235 / 300] ------------------------------- 2.7971\n",
            "[Epoch 236 / 300] ------------------------------- 2.8073\n",
            "[Epoch 237 / 300] ------------------------------- 2.8161\n",
            "[Epoch 238 / 300] ------------------------------- 2.7538\n",
            "[Epoch 239 / 300] ------------------------------- 2.6565\n",
            "[Epoch 240 / 300] ------------------------------- 2.6846\n",
            "[Epoch 241 / 300] ------------------------------- 2.6550\n",
            "[Epoch 242 / 300] ------------------------------- 2.7883\n",
            "[Epoch 243 / 300] ------------------------------- 2.7187\n",
            "[Epoch 244 / 300] ------------------------------- 2.6760\n",
            "[Epoch 245 / 300] ------------------------------- 2.7029\n",
            "[Epoch 246 / 300] ------------------------------- 2.7158\n",
            "[Epoch 247 / 300] ------------------------------- 2.6501\n",
            "[Epoch 248 / 300] ------------------------------- 2.6243\n",
            "[Epoch 249 / 300] ------------------------------- 2.6924\n",
            "[Epoch 250 / 300] ------------------------------- 2.7193\n",
            "[Epoch 251 / 300] ------------------------------- 2.7098\n",
            "[Epoch 252 / 300] ------------------------------- 2.6306\n",
            "[Epoch 253 / 300] ------------------------------- 2.7232\n",
            "[Epoch 254 / 300] ------------------------------- 2.7053\n",
            "[Epoch 255 / 300] ------------------------------- 2.6244\n",
            "[Epoch 256 / 300] ------------------------------- 2.6154\n",
            "[Epoch 257 / 300] ------------------------------- 2.7304\n",
            "[Epoch 258 / 300] ------------------------------- 2.6130\n",
            "[Epoch 259 / 300] ------------------------------- 2.5988\n",
            "[Epoch 260 / 300] ------------------------------- 2.6038\n",
            "[Epoch 261 / 300] ------------------------------- 2.6497\n",
            "[Epoch 262 / 300] ------------------------------- 2.6749\n",
            "[Epoch 263 / 300] ------------------------------- 2.6801\n",
            "[Epoch 264 / 300] ------------------------------- 2.5956\n",
            "[Epoch 265 / 300] ------------------------------- 2.6154\n",
            "[Epoch 266 / 300] ------------------------------- 2.6246\n",
            "[Epoch 267 / 300] ------------------------------- 2.6318\n",
            "[Epoch 268 / 300] ------------------------------- 2.6263\n",
            "[Epoch 269 / 300] ------------------------------- 2.7011\n",
            "[Epoch 270 / 300] ------------------------------- 2.6824\n",
            "[Epoch 271 / 300] ------------------------------- 2.6140\n",
            "[Epoch 272 / 300] ------------------------------- 2.6283\n",
            "[Epoch 273 / 300] ------------------------------- 2.7515\n",
            "[Epoch 274 / 300] ------------------------------- 2.6400\n",
            "[Epoch 275 / 300] ------------------------------- 2.6077\n",
            "[Epoch 276 / 300] ------------------------------- 2.5445\n",
            "[Epoch 277 / 300] ------------------------------- 2.5421\n",
            "[Epoch 278 / 300] ------------------------------- 2.7367\n",
            "[Epoch 279 / 300] ------------------------------- 2.6057\n",
            "[Epoch 280 / 300] ------------------------------- 2.6962\n",
            "[Epoch 281 / 300] ------------------------------- 2.5245\n",
            "[Epoch 282 / 300] ------------------------------- 2.5124\n",
            "[Epoch 283 / 300] ------------------------------- 2.5880\n",
            "[Epoch 284 / 300] ------------------------------- 2.7257\n",
            "[Epoch 285 / 300] ------------------------------- 2.5239\n",
            "[Epoch 286 / 300] ------------------------------- 2.5827\n",
            "[Epoch 287 / 300] ------------------------------- 2.7070\n",
            "[Epoch 288 / 300] ------------------------------- 2.6247\n",
            "[Epoch 289 / 300] ------------------------------- 2.5158\n",
            "[Epoch 290 / 300] ------------------------------- 2.5123\n",
            "[Epoch 291 / 300] ------------------------------- 2.6195\n",
            "[Epoch 292 / 300] ------------------------------- 2.5693\n",
            "[Epoch 293 / 300] ------------------------------- 2.7009\n",
            "[Epoch 294 / 300] ------------------------------- 2.4771\n",
            "[Epoch 295 / 300] ------------------------------- 2.6066\n",
            "[Epoch 296 / 300] ------------------------------- 2.5611\n",
            "[Epoch 297 / 300] ------------------------------- 2.4992\n",
            "[Epoch 298 / 300] ------------------------------- 2.4819\n",
            "[Epoch 299 / 300] ------------------------------- 2.4611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    ('voce nao pode falar nada', \"you can not say anything\"),\n",
        "    ('existe um almoco no armario',\"there is a meal in the shelve\")\n",
        "]\n",
        "\n",
        "dataset = CustomDataset([pair[0] for pair in test_data], [pair[1] for pair in test_data], source_vocab, target_vocab,max_sentence_len)\n",
        "test_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for test_sentences,targets in test_loader:\n",
        "\n",
        "    model.eval()\n",
        "    torch.no_grad()\n",
        "\n",
        "    batch_size = test_sentences.shape[1]\n",
        "    target_len = target.shape[0]\n",
        "\n",
        "    for idx in range(0,len(test_sentences)):\n",
        "\n",
        "        outputs = torch.zeros(16, 1, input_dim_decoder)\n",
        "\n",
        "        for i in range(0,16):\n",
        "            outputs[i] = target_vocab['<sos>']\n",
        "\n",
        "        hidden, cell = encoder(test_sentences[idx])\n",
        "\n",
        "        x = test_sentences[idx][0]\n",
        "\n",
        "        translated_sentence = []\n",
        "\n",
        "        for t in range(1, 16):\n",
        "\n",
        "            output, hidden, cell = decoder(x, hidden, cell)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            next_word = output.argmax(0)\n",
        "\n",
        "            x = next_word\n",
        "\n",
        "            translated_sentence.append(x)\n",
        "\n",
        "        translated_sentence2 = []\n",
        "        for idx in translated_sentence:\n",
        "            translated_sentence2.append(target_idx2word[int(idx)])\n",
        "\n",
        "        print(''.join(translated_sentence2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8yfy9IFUO1p",
        "outputId": "431bdc32-50a0-4760-e4be-3de90dfe8c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos><sos><sos><sos><sos>the<sos><sos>cooks<sos>cooks<sos>cooks<sos>cooks\n",
            "<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\n"
          ]
        }
      ]
    }
  ]
}